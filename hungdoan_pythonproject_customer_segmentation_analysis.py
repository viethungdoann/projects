# -*- coding: utf-8 -*-
"""HungDoan_PythonProject_Customer Segmentation Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xz7hDM7Z-1DUxT4IHjNOxLnt4hwgyr35

# **CUSTOMER SEGMENTATION ANALYSIS FOR MARKETING CAMPAIGNS**

Customer Personality Analysis is like getting to know your friends deeply. It's when a business takes time to really understand what each customer likes and doesn't like. This way, the business can make products that fit just right, like picking out a gift for a close friend.

Instead of inviting the whole town to a party, you only send invites to the friends who love the theme. That's what businesses do with their products. They figure out which customers are most likely to enjoy what they're selling, then they show the product to just those people.

# **Dictionary**

**People**

- ID: Customer's unique identifier
- Year_Birth: Customer's birth year
- Education: Customer's education level
- Marital_Status: Customer's marital status
- Income: Customer's yearly household income
- Kidhome: Number of children in customer's household
- Teenhome: Number of teenagers in customer's household
- Dt_Customer: Date of customer's enrollment with the company
- Recency: Number of days since customer's last purchase
- Complain: 1 if the customer complained in the last 2 years, 0 otherwise

**Products**

- MntWines: Amount spent on wine in last 2 years
- MntFruits: Amount spent on fruits in last 2 years
- MntMeatProducts: Amount spent on meat in last 2 years
- MntFishProducts: Amount spent on fish in last 2 years
- MntSweetProducts: Amount spent on sweets in last 2 years
- MntGoldProds: Amount spent on gold in last 2 years

**Promotion**

- NumDealsPurchases: Number of purchases made with a discount
- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise
- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise
- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise
- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise
- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise
- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise
Place
- NumWebPurchases: Number of purchases made through the company’s website
- NumCatalogPurchases: Number of purchases made using a catalogue
- NumStorePurchases: Number of purchases made directly in stores
- NumWebVisitsMonth: Number of visits to company’s website in the last month

**Target**
- Need to perform clustering to summarize customer segments
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans
np.random.seed(24)

df=pd.read_csv('marketing_campaign_segmentation.csv')
df.head()

"""# **DATA CLEANING & PROCESSING**"""

print(df.isna().sum())
print(df.duplicated().sum())

#Convert year_birth to age

from datetime import datetime

# Get the current year
current_year = datetime.now().year

# Calculate the age
df['Age'] = current_year - df['Year_Birth']

# Create new columns
df['Children'] = df['Kidhome'] + df['Teenhome']
df['is_parent'] = np.where(df['Children']>0,1,0)

#Rename columns
df=df.rename(columns={"MntWines": "Wines","MntFruits":"Fruits","MntMeatProducts":"Meat","MntFishProducts":"Fish","MntSweetProducts":"Sweets","MntGoldProds":"Gold"})



# Define a function that maps the current education to the new group
def map_education_group(education):
    if education == 'Graduation':
        return 'Graduate'
    elif education in ['2n Cycle', 'Basic']:
        return 'Undergraduate'
    elif education in ['PhD', 'Master']:
        return 'Postgraduate'
    else:
        return 'Other'
# Apply the function to the 'Education' column
df['Education'] = df['Education'].apply(map_education_group)
# Check the new value counts
print(df['Education'].value_counts())


# Define a function to map marital statuses to groups
def map_marital_status(status):
    if status in ['Married', 'Together']:
        return 'Partner'
    else:
        return 'Alone'
# Apply the function to the 'Marital_Status' column
df['Marital_Status_Group'] = df['Marital_Status'].apply(map_marital_status)
# Check the new value counts
print(df['Marital_Status_Group'].value_counts())

#drop unecessary columns
df=df.drop(columns=['Unnamed: 0','Marital_Status','ID','Teenhome','Dt_Customer','Kidhome','Year_Birth','Z_CostContact','Z_Revenue','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5'])

#check for columns with missing values
df.isna().sum()

#drop missing values
df=df.dropna()

#check for duplicated rows
df.duplicated().sum()

#drop duplicated rows
df=df.drop_duplicates()

df.head()

#Create dummy variables
encoded_df = pd.get_dummies(df, columns=['Education','Marital_Status_Group'])

encoded_df.head()

encoded_columns = ['Complain','Response','is_parent','Education_Graduate','Education_Postgraduate','Education_Undergraduate','Marital_Status_Group_Alone','Marital_Status_Group_Partner']
#change type of columns to category:
encoded_df[encoded_columns]=encoded_df[encoded_columns].astype('category')

encoded_df.info()

"""# **EDA**

###**Distribution of purchased products**
"""

# Distribution of purchased products in last 2 years
product_columns = ['Wines', 'Fruits', 'Meat', 'Fish', 'Sweets', 'Gold']

# Calculating the total count for each product category
product_totals = encoded_df[product_columns].sum()

# Creating a barplot
plt.figure(figsize=(10, 6))
sns.barplot(x=product_totals.index, y=product_totals.values, palette="flare")
plt.title('Total Count of Products Purchased')
plt.ylabel('Total Count')
plt.xlabel('Product Categories')
plt.show()

"""###**Distribution by educational background**"""

education_counts = df['Education'].value_counts()

# Creating a barplot
plt.figure(figsize=(8, 6))
sns.barplot(x=education_counts.index, y=education_counts.values, palette="flare")
plt.title('Distribution by educational background')
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.show()

"""### **Distribution by marital status**"""

marital_status_counts = df['Marital_Status_Group'].value_counts()

# Creating a barplot
plt.figure(figsize=(8, 6))
sns.barplot(x=marital_status_counts.index, y=marital_status_counts.values, palette="flare")
plt.title('Distribution by marital status')
plt.xlabel('Marital status')
plt.ylabel('Count')
plt.show()

"""### **Correlation matrix**"""

#Correlation heatmap
plt.figure(figsize=(10,10))
sns.heatmap(encoded_df.corr(), annot=True)
plt.show()

"""###**Skewness**"""

#check skewness
skewness = encoded_df.skew()
print("Skewness of each column:")
print(skewness)

#fix skewness by applying logarithmic transformation
encoded_df['Income_log'] = np.log(encoded_df['Income'] + 1)  # Adding 1 to avoid log(0)

"""# **Implementing PCA to reduce number of dimensions**"""

#Standardize all numeric variables to implment PCA

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# List of columns to be scaled
columns_to_scale = ['Recency','Wines','Fruits','Meat','Fish','Sweets','Gold','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Age','Children','Income_log']  # replace with your actual column names

# Apply scaling only to the specified columns
standardized_df= encoded_df.copy()
standardized_df[columns_to_scale] = scaler.fit_transform(standardized_df[columns_to_scale])

standardized_df=standardized_df.drop(columns='Income',axis=1)

#Initiating PCA to reduce dimentions aka features to 3
pca = PCA(n_components=3)
pca.fit(standardized_df)
pca_df = pd.DataFrame(pca.transform(standardized_df), columns=(["col1","col2", "col3"]))
pca_df.describe().T

pca_df.head()

"""# **Implementing clustering to group customers with like characteristics**"""

#Use elbow method to determine number of clusters
from yellowbrick.cluster import KElbowVisualizer
elbow = KElbowVisualizer(KMeans(), k=10)
elbow.fit(pca_df)
elbow.show()

from sklearn.cluster import AgglomerativeClustering
#Initiating the Agglomerative Clustering model
AC = AgglomerativeClustering(n_clusters=4)

# fit model and predict clusters
ac_clustering = AC.fit_predict(pca_df)
pca_df["Clusters"] = ac_clustering

#Adding the Clusters feature to the orignal dataframe.
df["Clusters"]= ac_clustering

#Prepare axis for clusters
x= pca_df['col1']
y= pca_df['col2']
z= pca_df['col3']

#Plotting the clusters
fig = plt.figure(figsize=(10,8))
ax = plt.subplot(111, projection='3d', label="bla")
ax.scatter(x, y, z, s=40, c=pca_df["Clusters"], marker='o',cmap='flare' )
ax.set_title("Clusters in 3D dimensions")
plt.savefig('clusters.png')
plt.show()

#Show orginal df
df.head()

#Create new column to prepare for visualizing clusters
df['total_purchases'] = df['Wines']+df['Fruits']+df['Meat']+df['Fish']+df['Sweets']+df['Gold']

scatter_clusters = sns.scatterplot(data = df,x=df['total_purchases'], y=df["Income"],hue=df["Clusters"],palette='flare')
scatter_clusters.set_title("Distribution of clusters based on income and total_purchases")
plt.legend()
plt.show()

plt.figure(figsize=(14, 6))

# First subplot for total_purchases
plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot
sns.violinplot(x='Clusters', y='total_purchases', data=df,cmap='flare')
plt.title('Distribution of Total Purchases by Cluster')

# Second subplot for Income
plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot
sns.violinplot(x='Clusters', y='Income', data=df,cmap='flare')
plt.title('Distribution of Income by Cluster')

# Show plot
plt.tight_layout()  # Adjusts the plots to make sure there is no overlap
plt.show()

# Create a boxplot to visualize outliers for each cluster based on Income
plt.figure(figsize=(10, 6))
sns.boxplot(x='Clusters', y='Income', data=df)

plt.title('Outliers of Income in Each Cluster')
plt.xlabel('Cluster')
plt.ylabel('Income')
plt.show()

"""**Cluster 0 seems to be the most promising group of customers based on the number of total purchases and the income ranges. This group is described as high-income and frequent shoppers.**

# **Predictive models**
"""

df.head()

from sklearn.feature_selection import mutual_info_regression

# Assuming 'encoded_df' is your DataFrame and 'Response' is the name of your target column.

# Split the dataset into features and target
X = encoded_df.drop('Response', axis=1)  # Drop the target column to get the features
y = encoded_df['Response']  # The target variable

# Apply Information Gain
ig = mutual_info_regression(X, y)

# Create a dictionary of feature importance scores
feature_scores = {X.columns[i]: ig[i] for i in range(len(ig))}  # Use len(ig) for the correct length

# Sort the features by importance score in descending order
sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)

# Print the feature importance scores and the sorted features
for feature, score in sorted_features:
    print("Feature:", feature, "Score:", score)

# Plot a horizontal bar chart of the feature importance scores
fig, ax = plt.subplots()
y_pos = np.arange(len(sorted_features))
ax.barh(y_pos, [score for feature, score in sorted_features], align="center")
ax.set_yticks(y_pos)
ax.set_yticklabels([feature for feature, score in sorted_features])
ax.invert_yaxis()  # Labels read top-to-bottom
ax.set_xlabel("Importance Score")
ax.set_title("Feature Importance Scores (Information Gain)")

# Add importance scores as labels on the horizontal bar chart
for i, v in enumerate([score for feature, score in sorted_features]):
    ax.text(v + 0.01, i, str(round(v, 3)), color="black", fontweight="bold")

plt.show()

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

X = encoded_df[['Wines','NumCatalogPurchases','Meat','Recency','NumStorePurchases','Education_Postgraduate']] # Drop the target column to get the features
y = encoded_df['Response']  # The target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)


# Initialize Gaussian Naive Bayes
gnb = GaussianNB()

# Fit the model on the training data
gnb.fit(X_train, y_train)

# Predict the responses for the test set
y_pred = gnb.predict(X_test)

# Evaluate the model's performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier

# Split the dataset into features and target
X = encoded_df[['Wines','NumCatalogPurchases','Meat','Recency','NumStorePurchases','Education_Postgraduate']]  # Features
y = encoded_df['Response']  # Target

# Split the dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)

# Initialize Random Forest classifier with n_estimators (number of trees)
rfc = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model on the training data
rfc.fit(X_train, y_train)

# Predict the responses for the test set
y_pred = rfc.predict(X_test)

# Evaluate the model's performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

!pip install scikit-learn imbalanced-learn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer



# Split the dataset into features and target
X = encoded_df[['Wines', 'NumCatalogPurchases', 'Meat', 'Recency', 'NumStorePurchases', 'Education_Postgraduate']]
y = encoded_df['Response']

# Split the dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)

# Create an imputer object with a median filling strategy
imputer = SimpleImputer(strategy='median')

# Impute missing values in X_train and X_test
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Apply SMOTE to the imputed training data
smote = SMOTE(random_state=24)
X_train_smote, y_train_smote = smote.fit_resample(X_train_imputed, y_train)

# Initialize Random Forest classifier with n_estimators (number of trees)
rfc = RandomForestClassifier(n_estimators=100, random_state=24)

# Fit the model on the training data after applying SMOTE
rfc.fit(X_train_smote, y_train_smote)

# Predict the responses for the test set
y_pred = rfc.predict(X_test_imputed)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the results
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

from sklearn.tree import DecisionTreeClassifier


X = encoded_df[['Wines','NumCatalogPurchases','Meat','Recency','NumStorePurchases','Education_Postgraduate']]  # Features
y = encoded_df['Response']  # Target


# Split the dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=22)

# Fit the classifier to the training data
clf.fit(X_train, y_train)

# Predict on the test data
y_pred = clf.predict(X_test)

# Calculate accuracy and display the classification report and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Evaluate the model's performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))